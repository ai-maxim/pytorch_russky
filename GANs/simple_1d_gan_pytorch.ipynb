{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генератор одномерных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном семинаре мы рассмотрим простой пример применения генеративно-состязательных сетей.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/model.png\" width=320px height=240px>\n",
    "\n",
    "Простым примером являются генеративно-состязательные сети, которые обучатся преобразованию одномерного равномерного распределения одномерное нормальное распределение. \n",
    "\n",
    "Данный пример содержит множество полезных визуализаций, которые помогут изучить работу двух сетей и исправить распространённые ошибки без необходимости ждать окончания многочасовых расчётов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def sample_noise(batch_size):\n",
    "    \"\"\" Uniform noise of shape [batch_size, 1] in range [0, 1]\"\"\"\n",
    "    return Variable(torch.rand(batch_size, 1))\n",
    "\n",
    "def sample_real_data(batch_size):\n",
    "    \"\"\" Normal noise of shape [batch_size, 1], mu=5, std=1.5 \"\"\"\n",
    "    return Variable(torch.randn(batch_size, 1) * 1.5 + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator converts 1d noise into 1d data\n",
    "gen = nn.Sequential(nn.Linear(1, 16), nn.ELU(), nn.Linear(16, 1))\n",
    "gen_opt = torch.optim.SGD(gen.parameters(), lr=1e-3)\n",
    "\n",
    "# Discriminator converts 1d data into two logits (0th for real, 1st for fake). \n",
    "# It is deliberately made stronger than generator to make sure disc is slightly \"ahead in the game\".\n",
    "disc = nn.Sequential(nn.Linear(1, 64), nn.ELU(), nn.Linear(64, 2), nn.Sigmoid())\n",
    "disc_opt = torch.optim.SGD(disc.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we define 0-th output of discriminator as \"is_fake\" output and 1-st as \"is_real\"\n",
    "IS_FAKE, IS_REAL = 0, 1\n",
    "\n",
    "def train_disc(batch_size):\n",
    "    \"\"\" trains discriminator for one step \"\"\"\n",
    "    \n",
    "    # compute p(real | x)\n",
    "    real_data = sample_real_data(batch_size)\n",
    "    p_real_is_real = disc(real_data)\n",
    "    \n",
    "    # compute p(fake | G(z)). We detach to avoid computing gradinents through G(z)\n",
    "    noise = <sample noise>\n",
    "    gen_data = <generate data given noise>\n",
    "    p_gen_is_fake = <compute logp for 0th>\n",
    "    \n",
    "    disc_loss = <compute loss>\n",
    "    \n",
    "    # sgd step. We zero_grad first to clear any gradients left from generator training\n",
    "    disc_opt.zero_grad()\n",
    "    disc_loss.backward()\n",
    "    disc_opt.step()\n",
    "    return disc_loss.data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gen(batch_size):\n",
    "    \"\"\" trains generator for one step \"\"\"\n",
    "        \n",
    "    # compute p(fake | G(z)).\n",
    "    noise = <sample noise>\n",
    "    gen_data = <generate data given noise>\n",
    "    \n",
    "    p_gen_is_real = <compute logp gen is REAL>\n",
    "    \n",
    "    gen_loss = <generator loss>\n",
    "    \n",
    "    gen_opt.zero_grad()\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()\n",
    "    return gen_loss.data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(100000):\n",
    "\n",
    "    for _ in range(5):\n",
    "        train_disc(128)\n",
    "    \n",
    "    train_gen(128)\n",
    "    \n",
    "    if i % 250 == 0:\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=[14, 6])\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Data distributions\")\n",
    "        plt.hist(gen(sample_noise(1000)).data.numpy()[:, 0], range=[0, 10], alpha=0.5, normed=True, label='gen')\n",
    "        plt.hist(sample_real_data(1000).data.numpy()[:,0], range=[0, 10], alpha=0.5, normed=True, label='real')\n",
    "        \n",
    "        x = np.linspace(0,10, dtype='float32')\n",
    "        disc_preal = disc(Variable(torch.from_numpy(x[:, None])))\n",
    "        plt.plot(x, disc_preal.data.numpy(), label='disc P(real)')\n",
    "        plt.legend()\n",
    "        \n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Discriminator readout on real vs gen\")\n",
    "        plt.hist(disc(gen(sample_noise(100))).data.numpy(),\n",
    "                 range=[0, 1], alpha=0.5, label='D(G(z))')\n",
    "        plt.hist(disc(sample_real_data(100)).data.numpy(),\n",
    "                 range=[0, 1], alpha=0.5, label='D(x)')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Что ожидать:__\n",
    "* __Слева:__ Поначалу два распределения будут выглядеть по-разному, но потом распределение генератора должно совпасть с реальными данными _почти_ везде. Кривая представляет собой решение дискриминатора для всех возможных значений x. Постепенно оно должно приближаться к 0.5 в областях, содержащих много реальных значений.\n",
    "* __Справа:__ Этот график показывает, как часто дискриминатор назначает определённое значение вероятности истинным и искуственным образцам (они отличаются цветом). Для первых итераций допустимы колебания. После этого, когда генератор достаточно обучится, большая часть значений приблизится к 0.5.\n",
    " * Если вместо этого значения сходятся к двум дельта-функциям около 0 (искусственные образцы) и 1 (истинные образцы), то дискриминатор одержал верх над генератором. _Проверьте функцию потерь генератора_. Другим решением является уменьшение скорости обучения дискриминатора. Это также может произойти при замене среднего для батча на сумму или схожую функцию.\n",
    " * Если значения сходятся к 0.5 и остаются там на протяжении нескольких итераций, но генератор не научился генерировать правдоподобные образцы, то генератор одержал верх над дискриминатором. _Дополнительно проверьте функцию потерь дискриминатора_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
